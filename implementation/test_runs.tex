When developing a program for numerical simulations from scratch, it is crucial to design test cases and benchmarks to verify the goodness of the implementation. The easiest case is that of a fully deterministic program (for which the output depends only on some initial parameters) because one can compare the results with other implementations to validate the code. In the case of stochastic processes, like the Metropolis algorithm, it is only possible to compare the average behavior with external implementations. \\
In this chapter we will give an overview of the test cases, the properties of the ensembles used in this thesis and how the parameters that were needed to generate them have been set.

\section{Generated Ensembles}
In order to study the momentum scale it has been necessary to choose a set of decreasing lattice spacings to then be able to take the continuum limit. This procedure is common to most Lattice QCD calculations as it is the most straightforward way to take recover the continuum theory. The lattice spacing is linked to the coupling $g_0$ and $\beta$ by the relation found in \cref{scale:parameter}.\\ 
We chose 4 values of $\beta$ that span lattice spacings from approximately $0.1$ fm to $0.05$ fm in approximately equal steps. \\
For the calculation to be consistent however, the total volume of the lattice should be kept constant, so the choice of the lattice spacings also determined the number of lattice sites per dimension, having $L = aN\approx const$. The time dimension has been take to be twice as big as the space dimension. The number of lattice points per dimension, for parallelization's sake, were chosen in order to have many divisors, to allow for different and small sub-blocks. \\
Table \cref{runs:ensembles} summarizes the physical properties of the ensembles that were generated.
\begin{table}[!htb]
    \begin{center}
    \begin{tabular}{cccc}
        $\beta$ & $a$ & $N^3\times T$ & $aL$ [fm]\\\hline
        $6.00$ & $0.09314$ & $24^3 \times 48$ & $2.23536$\\
        $6.10$ & $0.07905$ & $28^3 \times 56$ & $2.21367$\\
        $6.20$ & $0.06793$ & $32^3 \times 64$ & $2.17405$\\
        $6.45$ & $0.04781$ & $48^3 \times 96$ & $2.29488$\\
    \end{tabular}
    \label{runs:ensembles}
    \capt{Physical properties of the ensembles used for this work.}
    \end{center}
\end{table}
For each value of $\beta$ a statistical ensemble was needed. Ideally, one would take as many configurations as possible for each value and in principle one would have the same number of configurations for each of them. However it is clear from the discussion in \cref{chap:code_design} that the number of lattice sites affects computation times and the capability of storing the configurations dramatically. Moreover, as will be shown in \cref{sec:obs_autocorr} the autocorrelation time of the observables has a non-trivial, power-law or exponential, behavior with the lattice spacing, making the generation of the larger $\beta$ ensembles even more time consuming.\\
The following table summarizes the final values for the parameters of the Metropolis algorithm for the different ensembles.
\begin{table}[!htb]
    \begin{center}
    \begin{tabular}{cccccc}
        $\beta$ & $N_{conf}$ & $N_{corr}$ & MC Steps & $N_{hit}$ & $\epsilon_{SU(3)}$\\\hline
        $6.00$ & $1000$ & $600$ & $600000$ & $30$ & $0.25$\\
        $6.10$ & $500$ & $600$ & $300000$ & $30$ & $0.25$\\
        $6.20$ & $500$ & $800$ & $400000$ & $30$ & $0.25$\\
        $6.45$ & $250$ & $1600$ & $400000$ & $30$ & $0.25$
    \end{tabular}
    \label{runs:mcparams} 
    \capt{Parameters used for the generation of the ensembles on which the results of this work are based on.}
    \end{center}
\end{table}
These values have been chosen after many tests, checks of the autocorrelation time and mainly in an empirical way. There are some parameters that are free in principle and no real reference study on their impact on the resulting ensemble. In the next section the trial and error approach that led to this decision will be briefly discussed.\\
Evidently, there is less freedom on the choice of the parameters in \cref{FLOW:params} for the gradient flow compared to \cref{MC:params}. The value of $t_f^{MAX}$ is chosen in lattice units. To rescale it to physical units it must be multiplied by $a^2$. In the following table we report the choice of the parameters for the flow of the ensembles in \cref{runs:ensembles}.

\begin{table}[!htb]
    \begin{center}
    \begin{tabular}{cccc} 
        $\beta$ & $\epsilon$ & $t_f^{MAX}/a^2$ & $\sqrt{t_f^{MAX}}$ [fm] \\\hline
        $6.00$ & $0.01$ & $10.00$ & $0.8330$ \\
        $6.10$ & $0.01$ & $10.00$ & $0.7070$ \\
        $6.20$ & $0.01$ & $10.00$ & $0.6076$ \\
        $6.45$ & $0.02$ & $20.00$ & $0.6047$ 
    \end{tabular}
    \label{runs:flow} 
    \capt{Parameters used for the numerical integration of the flow equations for the ensembles in \cref{runs:ensembles}}
    \end{center}
\end{table}
The choice of $\epsilon$ was motivated by the work found in \cite{ce_testing_2015}, and the same values for the combination of $\epsilon$ and $t_f^{MAX}/a^2$ have been chosen for all ensembles, regardless of the physical maximum smearing radius, except for the $\beta=6.45$ case, for which the number of integration steps of the RK-MK method is kept the same, but the step size is doubled. This was needed to obtain results for large enough flow times out of the ensemble (which is by far the largest one) in reasonable computing time.


\section{Test Runs}
\label{sec:testautocorr}
Running some test calculations with a completely new program is obviously necessary. First some benchmarks to check the expectation values of the observables, both on raw configurations and on flowed configurations. These benchmarks were made using 2 configurations generated with the CHROMA \cite{edwards_chroma_2005} code base from the USQCD collaboration for zero flow time observables and one configuration flowed using an extension of CHROMA called FlowOps (courtesy of T.Luu and A.Shidler ?correct?), built on QDP++ (the backbone of CHROMA), that applies the Wilson flow to configurations. Both types of tests, once all the parameters were made equal, gave results equal to machine precision with the ones generated with the new code base.\\
Checking the validity of expectation values of observables is a solid indication that the overall back-end of the new code-base has been implemented well, as the results are deterministic. Testing the Metropolis algorithm and assessing the quality of the generated ensembles is much harder, because it involves stochastic computations, hence no numerical check can be easily defined, one can only look at average properties of the set of configurations. Unfortunately the generation of gauge field configuration as we saw in \cref{MC:params} has 3 parameters that need to be set that do affect the statistical properties of the output ensemble, mainly the autocorrelation. As it turns out however, the $\epsilon$ parameter, which controls the spread of the $SU(3)$ random elements around the identity matrix, only affects the acceptance ratio of the Metropolis test, hence the efficiency of the algorithm and not the physical properties (or at least the effects are small).\\
The tests were all performed on the three largest lattice spacings, as the system for the smallest one is much larger and requires a considerably longer time to compute.

\subsection{Thermalization}
\label{sec:thermalization}
When initializing the Markov Chain, to generate gauge field configurations, one has two options for the initial condition of the link variables of the lattice:
\begin{itemize}
    \item Hot Start: all links are set to the $SU(3)$ identity element
    \item Cold Start: all links are set to random $SU(3)$ matrices.
\end{itemize}
It is easy to see that for the hot start the initial value for the plaquette operator would be 1, when normalized on the lattice volume. For the cold start the initial value is a random number, centered around 0. The Metropolis algorithm is supposed to drive the Markov chain towards the peak of the PDF, \cref{eq:PDF}, that is where the action has a minimum. \\
When computing expectation values using the Monte Carlo integration, it is important to ensure that the sampling of the observables is performed when the Metropolis algorithm reached equilibrium, when it is thermalized. In \cref{fig:thermalization} we show that only a small number of MC updates is needed to reach equilibrium (compare the thermalization time of $\approx 600$ with the value of $N_{corr}$ from \cref{MC:params} and the discussion in \cref{sec:obs_autocorr}). 
\fig[0.8]{implementation/ThermPlaq.pdf}{Expectation value of the Plaquette as a function of the Monte Carlo updates performed, for hot and cold starts. Equilibrium is reached in both cases after $\approx 600$ updates. The data is taken from a simulation with $\beta=6.10$.}{fig:thermalization}
To ensure thermalization, all results of this work are generated discarding the first $10^4$ MC cycles, which is much larger than the value we found, but still a small fraction, so an affordable exaggeration, of the total number of MC cycles of the Markov chain.


\subsection{Tests for the Autocorrelation of Observables}
\label{sec:obs_autocorr}
The most important test has been the assessment of the autocorrelation time for different observables at various lattice spacings varying the parameters of the generation algorithm. As expected, the most problematic quantity is the topological charge, especially at large flow times. A good measure of how much a data series is autocorrelated is the integrated autocorrelation time $\tau_{int}$, see \cref{app:autocorr}, which is expected to be $1/2$ if the data is uncorrelated. In general, a larger $\tau_{int}$ implies an underestimation of uncertainties, so the variance of a quantity is corrected as $\tilde\sigma^2 = 2\tau_{int}\sigma^2$. \\
The first test, in \cref{fig:autobetas} shows the integrated autocorrelation time for the topological charge at fixed $N_{corr}=200$ and $N_{hit} = 10$ for different lattice spacings:
\fig[0.8]{implementation/TopcAutoBetas.pdf}{Integrated autocorrelation time for $N_{corr}=200$ and $N_{hit} = 10$ for different inverse coupling values. }{fig:autobetas}

Afterwards we looked at a single lattice spacing and tried to vary the parameters $N_{corr}$ and $N_{hits}$ for the $\beta=6.10$ case. 
\fig[0.8]{implementation/TopcTauInt.pdf}{Integrated autocorrelation time for $\beta=6.10$ as the parameters for the gauge field generation $N_{corr}$ and $N_{hit}$ are modified. The black data series represents the chosen set of parameters. }{fig:autotauint}
From the first plot it is clear that the values that were guessed for the parameters $N_{corr}$ and $N_{hit}$, which could be acceptable for the lowest beta value, are not at all fine for the other lattice spacings. On a general note we observe how the flow quickly removes the low range noise from the observable through the smearing, making it clear that the topological charge is highly correlated in Monte Carlo time even though it might seem uncorrelated if one only looks at $t_f = 0$. \\
On the other hand, \cref{fig:autotauint} suggests a possible cure for the autocorrelation issue. It is almost tautological that increasing the value of $N_{corr}$ decreases the autocorrelation, as the parameter represents how many MC updates are performed between two measurements of the observable. The dependence of $\tau_{int}$ from $N_{hits}$ is also clear. From the data we observed that increasing both parameters improved the autocorrelation, so the choice was to set both to a larger value from our initial guess and this proved to be the right choice. The black data series in  \cref{fig:autotauint} is the data that we used for our analysis, one can notice that it is still autocorrelated, but that can be handled with a reasonable correction to the variance on the data. \\
It is important to stress again that the autocorrelation is a huge problem mainly for the topological charge. The energy density operator is much less affected and for all the ensembles we generated the integrated autocorrelation time is never a value much greater that $1/2$ (see \cref{fig:energyautocorr}). 
 
\subsection{Strong and Weak Scaling}
One important thing to look at when developing a program intended to be highly parallelized are the scaling properties. We can now check these for the two sections of the code, the generation of configurations and the gradient flow. We will distinguish the analysis in the two usual quantities used in High Performance Computing: strong and weak scaling. \\
Strong scaling is the performance of a program as the total problem size is kept fixed and the number of parallel units, the number of processors in this case, increases. The parallel efficiency is computed as:
\beq
    \eta_s = \frac{t_0}{t_NN}\cdot 100\% 
\eeq 
where $t_0$ is the execution time for one unit of computation, $t_N$ the execution time for the case of $N$ units computing the same system size. In the case for perfect parallelization the efficiency is $100\%$ as an increase of some factor in the number of processors would decrease by the same factor the execution time. In real cases this is seldomly true, the execution time is in fact usually larger than expected. \\
The test was set up in this way:
\begin{itemize}
    \item Gauge Field Generation: one lattice of size $24^3\times 48$ has been randomly generated and $10^4$ MC cycles were performed. The system was split in sub-lattices of decreasing size, so to increase the number of processors.
    \item Gradient Flow: ten lattice of size $24^3\times 48$ have been flowed for a total of $1000$ integration steps each. Again, the sub-lattice size was decreased and the number of processor increased accordingly. 
\end{itemize}

\fig[0.7]{implementation/StrongScaling.pdf}{Strong scaling of the two parts of the program. The efficiency is normalized to the first point plotted, that is 64 processors.}{fig:strong_scaling}

Figure \ref{fig:strong_scaling} shows that the performance scales well with the number of processors, but the efficiency is dropping for very large processor numbers. This is expected, the reason is that the sub-lattice the surface to volume ratio of the sub-lattice makes the fraction of operations that require inter-processor communication become comparable to the "inner" points calculations. It is to be noted that smaller number of processors could not be used, as the processors in that case would have been all on the same node (the cluster used for testing has 28 cores per node), so any timing would have excluded the inter-node communication time, which is significantly higher than the inter-node one.

By weak scaling of a parallelized problem it is intended the measure of the performance as the local workload is kept fixed and the total volume of the problem is increased by adding processors. The efficiency is computed as:
\beq
\eta_W = \frac{t_0}{t_N}\cdot 100\% 
\eeq 
where $t_0$ is the execution time of the smallest case and $t_N$ the one of a system $N$ times larger computed on $N$ times the parallel units. The test set up was:
\begin{itemize}
    \item Gauge Field Generation: the sub-lattice has been fixed to $4^4$, and volumes from $16^3\times32$, a total of $32$ processes, to $32^3\times64$, $512$ processors, have been considered. Intermediate volumes are built by multiplying one spatial dimension at a time by a factor of 2. For each volume $10000$ MC cycles have been computed.
    \item Gradient Flow: the gauge fields generated in the previous point have been flowed for a total of $1000$ integration steps by the same number of processors that generated them, so the sub-lattice size was set again to be $4^4$.
\end{itemize}

\fig[0.7]{implementation/WeakScaling.pdf}{Strong scaling of the two parts of the program. The efficiency is the ratio with respect to the first data point. }{fig:weak_scaling}

The results in \cref{fig:weak_scaling} show that both the programs are perfectly scalable. The communication overhead, introduced by the need of sharing link variables across the edges, is constant with respect to the number of processors. This implies that the parallelization procedure has been implemented correctly, though it could still be potentially improved. \\
These results for strong and weak scaling are very comforting, as they show that the hardest part of the development process, the parallelization of the code, has been well designed and implemented.


\section{Main Runs and Timing}
All runs of the programs, to generate the four ensembles in \cref{runs:ensembles} and to apply and integrate the gradient flow equations to all their configurations, were carried out on the High Performance Computing Center at Michigan State University (MSU), with the support of the Institute for Cyber-Enabled Research (iCER). Development was performed on local machines and on the small cluster SMAUG located at the Department of Physics of the University of Oslo (UiO) and some larger benchmarks were run on the Abel Computer Cluster also at UiO.\\
The ensembles, on which the results in \cref{part:results} were generated using the parameters in table \cref{runs:mcparams}. The following table shows the computing resources used for the generation of all four ensembles. 

\begin{table}[!htb]
    \begin{center}
    \begin{tabular}{ccccc}
        $\beta$ & $N_{procs}$ & Wall Time [h] & Time per MC Cycle [s] & Time per Configuration [s]\\\hline
        $6.00$ & $1024$ & $27$ & $0.16104$  & $96.6$\\
        $6.10$ & $512$ & $35$ & $0.42102$ & $252.6$\\
        $6.20$ & $1024$ & $53$ & $0.4780825$ & $382.4$ \\
        $6.45$ & $1024$ & $204$ & $1.836$ & $2937.6$
    \end{tabular}
    \label{runs:times} 
    \capt{Execution times for generating ensembles from \cref{runs:ensembles} with parameters found in \cref{runs:mcparams}. All the runs were performed on the iCER cluster at MSU. $N_{procs}$ is the number of processors used for each ensemble; the Wall Time is the ``wall clock'' time (the time spent in the parallel execution); Time per MC cycle is the time spend in each step of the Markov Chain; Time per Configuration is the time spent between saved configurations, to be used for observables, so it includes the adjustment made for the longer autocorrelation times of larger $\beta$ values.} 
    \end{center}
\end{table}

It is clear from the table that the largest set, out of which only 250 configurations were saved, had execution times larger by one order of magnitude compared to the second largest; this made increasing the ensemble size unfeasible with the time and resources for this work.\\
Lastly we present the equivalent of \cref{runs:times} for the Gradient Flow program:

\begin{table}[!htb]
    \begin{center}
    \begin{tabular}{cccc}
        $\beta$ & $N_{procs}$ & Wall Time [h] & Time per Configuration [s]\\\hline
        $6.00$ & $1024$ & $15$ & $109.3$\\
        $6.10$ & $512$ & $30$ & $218.6$\\
        $6.20$ & $1024$ & $33$ & $243.6$ \\
        $6.45$ & $1024$ & $96$ & $1326.9$
    \end{tabular}
    \label{runs:times} 
    \capt{Execution times for the numerical integration of the flow equations on the ensembles from \cref{runs:flow} with parameters found in \cref{runs:flow}. All the runs were performed on the iCER cluster at MSU. $N_{procs}$ is the number of processors used for each ensemble; the Wall Time is the ``wall clock'' time (the time spent in the parallel execution); Time per Configuration is the time spent flowing one single gauge configuration.} 
    \end{center}
\end{table}