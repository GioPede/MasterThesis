When developing a program for numerical simulations from scratch, it is crucial to design test cases and benchmarks to verify the goodness of the implementation. The easiest case is that of a fully deterministic program (for which the output depends only on some initial parameters) because one can compare the results with other implementations to validate the code. In the case of stochastic processes, for example that of a Markov chain, it is only possible to compare between different implementations the average properties and results. \\
In this chapter we will give an overview of the test cases, that determined the validity of our code; the properties of the ensembles of gauge field configurations that have been used in this thesis, and how the numerical parameters that were needed to generate them have been set.

\section{Generated Ensembles}
In order to study the scale $\Lambda_{YM}$ of Pure Yang-Mills theory it has been necessary to choose a set of decreasing lattice spacings to then be able to take the continuum limit. This procedure is common to most Lattice QCD calculations as it is the most straightforward way to recover the continuum theory. The lattice spacing is linked to the coupling $g_0$, or the inverse coupling $\beta$, by the relation found in \cref{scale:parameter}.\\ 
We chose 4 values of $\beta$ that span lattice spacings from approximately $0.1$ fm to $0.05$ fm in approximately equal steps. \\
For the calculation to be consistent however, the total volume of the lattice should be kept constant, so the choice of the lattice spacings also determined the number of lattice sites per dimension, having $L = aN\approx const$. The time dimension has been take to be twice as big as the three spatial dimensions. The number of lattice points per dimension, for the sake of parallelization, were chosen in order to have many integer divisors, to allow for many different sizes of the sub-blocks to be set. Table \cref{runs:ensembles} summarizes the physical properties of the ensembles that were generated.

\begin{table}[!htb]
    %\captionsetup{justification=centering}
    \capt{Physical properties of the ensembles used for this work. The parameter $\beta$ is the inverse coupling, and its value fixed the lattice spacing $a$ through the usage of \cref{scale:parameter}. The value of $N$ represents the number of lattice sites per spatial dimension and $N_T$, here fixed to $2N$, is the number of sites in the time dimension. The size $L = aN$ of the system is also reported, in units of fermi.}
    \begin{center}
    \begin{tabular}{cccc}
        $\beta$ & $a$ & $N^3\times N_T$ & $L$ [fm]\\\hline
        $6.00$ & $0.09314$ & $24^3 \times 48$ & $2.23536$\\
        $6.10$ & $0.07905$ & $28^3 \times 56$ & $2.21367$\\
        $6.20$ & $0.06793$ & $32^3 \times 64$ & $2.17405$\\
        $6.45$ & $0.04781$ & $48^3 \times 96$ & $2.29488$\\
    \end{tabular}
    \label{runs:ensembles}
    \end{center}
\end{table}

For each value of $\beta$ a statistical ensemble of randomly chosen gauge field configurations was needed. Ideally, one would take as many configurations as possible for each value. However it is clear from the discussion in \cref{chap:code_design} that the number of lattice sites affects computation times and the capability of storing the configurations dramatically. Moreover, as will be shown in \cref{sec:obs_autocorr} the autocorrelation time of the observables has a non-trivial, power-law or exponential behavior with the lattice spacing, making the generation of the larger $\beta$ ensembles even more challenging.\\
In \cref{runs:mcparams} we report the final values for the parameters of the Metropolis-Hastings algorithm for the different ensembles that were generated.
\begin{table}[!htb]
    \capt{Parameters used for the generation of the ensembles on which the results of this work are based on. In the table $\beta$ is the inverse coupling; ``MC Cycles'' stands for the total number of Monte Carlo updates performed, a detailed description of an update is found in \cref{sec:update}. The following parameters are the algorithm specific ones described in \cref{MC:params}.}
    \begin{center}
    \begin{tabular}{cccccc}
        $\beta$ & MC Cycles & $N_{conf}$ & $N_{corr}$ &  $N_{hit}$ & $\epsilon_{SU(3)}$\\\hline
        $6.00$ & $600000$ & $1000$ & $600$ & $30$ & $0.25$\\
        $6.10$ & $300000$ & $500$ & $600$ & $30$ & $0.25$\\
        $6.20$ & $400000$ & $500$ & $800$ & $30$ & $0.25$\\
        $6.45$ & $400000$ & $250$ & $1600$ & $30$ & $0.25$
    \end{tabular}
    \label{runs:mcparams} 
    \end{center}
\end{table}

These values have been chosen after many tests, mainly checks of the autocorrelation time of the observables presented in \cref{sec:observables}. All the parameters are free in principle and no reference study on their impact on the resulting ensemble properties was found for the case of the Metropolis-Hastings algorithm. In the next section the trial and error approach that led to he choice of these parameters will be briefly discussed.\\
Evidently, there is less freedom on the choice of the parameters in \cref{FLOW:params} for the gradient flow compared to the gauge field generation. This is because the numerical integration of the flow equation is fully deterministic. There are only two quantities to fix: the value of $t_f^{MAX}$, the final flow time point; and the size of the integration step $\epsilon$. In the calculations the flow time is expressed in lattice units, $t_f \rightarrow t_f/a^2$, so the quantities we set were in this dimensionless form. To rescale the final flow time to physical units it must be multiplied again by the square of the lattice spacing, but it is more interesting to report the final smearing radius of the flow equation, remembering that it is defined as: $\sqrt{8t_f^{MAX}}$. In \cref{runs:flow} we report the choice of the parameters for the flow of the ensembles in \cref{runs:ensembles}.
 
\begin{table}[!htb]
    \capt{Parameters used for the numerical integration of the gradient flow equations \cref{lattice:flow} for the ensembles in \cref{runs:ensembles}. The inverse coupling is $\beta$ and its value implicitly defines the lattice spacing $a$. $\epsilon$ is the size of every Runge-Kutta integration step. The value of $t_f^{MAX}/a^2$ is the last point of integration in dimensionless units and the corresponding maximum smearing radius is shown as  $\sqrt{8t_f^{MAX}}$ in fermi.}
    \begin{center}
    \begin{tabular}{cccc} 
        $\beta$ & $\epsilon$ & $t_f^{MAX}/a^2$ & $\sqrt{8t_f^{MAX}}$ [fm] \\\hline
        $6.00$ & $0.01$ & $10.00$ & $0.8330$ \\
        $6.10$ & $0.01$ & $10.00$ & $0.7070$ \\
        $6.20$ & $0.01$ & $10.00$ & $0.6076$ \\
        $6.45$ & $0.02$ & $20.00$ & $0.6047$ 
    \end{tabular}
    \label{runs:flow} 
    \end{center} 
\end{table}
The choice of $\epsilon$ was motivated by the work by C\`{e} et al. \cite{ce_testing_2015}, and the same values for the combination of $\epsilon$ and $t_f^{MAX}/a^2$ have been chosen for all ensembles, regardless of the physical maximum smearing radius. The exception is the $\beta=6.45$ case, for which the total number of integration steps of the Runge-Kutta Munthe-Kaas integrator has been kept equal to the other lattice sizes, but the integration step length is doubled. This was needed to obtain results for large enough smearing radii out of the ensemble (which is by far the largest one) in reasonable computing time.


\section{Test Runs}
\label{sec:testautocorr}
Running some test calculations with a completely new program is obviously necessary. The simplest benchmark was to check the expectation values of the observables. To perform these checks we computed the energy density and the topological charge on two configurations generated with the CHROMA lattice QCD code from the USQCD collaboration\cite{edwards_chroma_2005}.  The results were found to match to machine precision. \\
The gradient flow implementation could also be tested numerically by comparing our results to the ones generated by an extension of CHROMA called FlowOps (courtesy of T.Luu and A.Shindler)\cite{shindler_nucleon_2015}. This program as well applies the gradient flow to gauge field configurations with the same integrator scheme that we used, \cref{eq:integrator}, thus by setting the same integration parameters we were able to compare the values of the observables at every flow time. Also in this case our implementation gave results equal to machine precision to the ones by FlowOps at all flow times for every configuration we tested.\\
Checking the validity of expectation values of observables is a solid indication that the overall core libraries of the new framework we developed has been implemented correctly. \\
Testing the algorithm to generate gauge field configurations and assessing the quality of the generated ensembles is much harder than testing the gradient flow, because it involves stochastic computations, hence no numerical check can be easily defined. One has instead to look at average properties of the set of configurations. The generation of gauge field configurations as we saw in \cref{MC:params} has three parameters that need to be set that do affect the statistical properties of the output ensemble, mainly the autocorrelation time of the observables. The simplest parameter to set was $\epsilon_{\mathrm{SU}(3)}$, which controls the spread of the $\mathrm{SU}(3)$ random elements around the identity matrix. This directly affects how much each link is changed from one MC cycle to the other, a larger value of $\epsilon_{\mathrm{SU}(3)}$ implies larger changes. The drawback is the decrease of the acceptance ratio of the Metropolis test and consequently of the efficiency of the program. The value that was chosen, $0.25$ was a good trade-off between the two things.\\
The other parameters two parameters to set were: $N_{corr}$, the number of configurations between observable measurements (in parctice between two saved configurations); ($N_{hits}$ the number of random unitary transformations applied to every link per MC cycle. Some tests runs with different combinations of the two were performed on the three largest lattice spacings, as the system for the smallest one is much larger and requires a considerably longer time to compute.


\subsection{Tests for the Autocorrelation of Observables}
\label{sec:obs_autocorr}
The most important test has been the assessment of the autocorrelation time for different observables at various lattice spacings varying the parameters of the ensemble generation algorithm. As expected, the most problematic quantity is the topological charge, especially at large flow times. A good measure of how much a data series is autocorrelated is the integrated autocorrelation time $\tau_{int}$, see \cref{app:autocorr}, which is expected to be $1/2$ if the data is uncorrelated. In general, a larger $\tau_{int}$ implies an underestimation of uncertainties, so the variance of a quantity is corrected as $\tilde\sigma^2 = 2\tau_{int}\sigma^2$. \\
The first test, in \cref{fig:autobetas} shows the integrated autocorrelation time for the topological charge at fixed $N_{corr}=200$ and $N_{hit} = 10$ for different lattice spacings:
\fig[0.7]{implementation/TopcAutoBetas.pdf}{Integrated autocorrelation time $\tau_{int}$ of the topological plotted against the smearing radius of the gradient flow $\sqrt{8t_f}$. The different ensembles shown have fixed values of $N_{corr}=200$ and $N_{hit} = 10$ and the different data series represent increasing values of the inverse coupling. The system sizes are the ones of \cref{runs:ensembles} for every value of $\beta$. }{fig:autobetas}

From the plot it is clear that the values that were guessed for the parameters $N_{corr}$ and $N_{hit}$, which could be acceptable for the lowest beta value, are not at all fine for the other lattice spacings. One can observe that for increasing values of $\beta$ the $\tau_{int}$ grows rapidly if the parameters of the generation algorithm are kept fixed.  This is the reason why in \cref{runs:ensembles} the different ensembles have different parameters, in particular growing $N_{corr}$ which represents the number of MC updates between two measurements of the  observables.\\
On a general note we see how the flow quickly removes the low range noise from the observable through the smearing effect, making it clear that the topological charge is highly correlated in Monte Carlo time even though it might seem uncorrelated if one only looks at $t_f = 0$. \\
A second test was to look at a single lattice spacing and try to vary the parameters $N_{corr}$ and $N_{hits}$. The results for the $\beta=6.10$ case are shown in . 
\fig[0.7]{implementation/TopcTauInt.pdf}{Integrated autocorrelation time of the topological charge as a function of the smearing radius of the gradient flow $\sqrt{8t_f}$. The ensembles have been generated with inverse coupling $\beta=6.10$ for a lattice of size $28^3\times58$. The different data series represent different combinations of the parameters for the gauge field generation $N_{corr}$ and $N_{hit}$. The black data series shows the set of parameters that were chosen for the generation of the larger ensemble (the one used for the analysis of the energy scale $\Lambda_{YM}$). }{fig:autotauint}
\\
The results of \cref{fig:autotauint} suggests a possible cure for the autocorrelation of the observables. It is almost tautological that increasing the value of $N_{corr}$ decreases the integrated autocorrelation time, as the parameter represents how many MC updates are performed between two measurements of the observable. The dependence of $\tau_{int}$ from $N_{hits}$ is also clear since the more attempts of transforming a single link are performed, the more the configuration will be different from the previous one. \\
From the plot we can observe that increasing both parameters decrease $tau_{int}$, so the choice was to set both to a larger value from our initial guesses. This proved to be the right choice. \\
The black data series in  \cref{fig:autotauint} is the data that we used for our analysis. One can notice that it is still autocorrelated, but to a degree that can be handled with a reasonable correction to the variance of the data. \\
It is important to stress again that autocorrelation is a huge problem mainly for the topological charge. The energy density operator is much less affected and for all the ensembles we generated the integrated autocorrelation time is never a value much greater that $1/2$ (see \cref{fig:energyautocorr}). 


\subsection{Thermalization}
\label{sec:thermalization}
When computing expectation values using Monte Carlo integration, it is important to ensure that the sampling of the observables is performed when the Metropolis-Hastings algorithm reached equilibrium, or in jargon when it is thermalized.\\
The Markov Chain to generate gauge field configurations can be initilized in two distinct ways. These represent the initial configuration of the gauge field to which Monte Carlo updates are later applied. The two possibilities are:
\begin{itemize} 
    \item Cold Start: all links of the gauge field configuration are set to the $\mathrm{SU}(3)$ identity element;
    \item Hot Start: all links are set to random $\mathrm{SU}(3)$ matrices.
\end{itemize}
It is easy to see that for the cold start the initial value for the plaquette operator, \cref{plaquette}, would be one, when normalized on the lattice volume and so the action starts in its minimum. For the hot start the initial value for the plaquette expectation value is a random number, centered around zero. The Metropolis-Hastings algorithm is supposed to drive the Markov chain towards the peak of the PDF, \cref{eq:PDF}, that corresponds to an intermediate value of the two. \\
\fig[0.7]{implementation/ThermPlaq.pdf}{Expectation value of the Plaquette as a function of the number Monte Carlo cycles performed, for hot and cold starts. Equilibrium is reached in both cases after $\approx 600$ updates. The data is taken from a simulation with $\beta=6.10$.}{fig:thermalization}

In \cref{fig:thermalization} we show that only a small number of MC updates is needed to reach equilibrium, especially when compared to the value of $N_{corr}$ from the discussion in \cref{sec:obs_autocorr}. \\
To ensure thermalization, all results of this work are generated discarding the first $10^4$ MC cycles, which is much larger than the value we found, but still a small fraction, so an affordable exaggeration, of the total number of MC cycles of the Markov chain.

\subsection{Strong and Weak Scaling}
One important thing to look at when developing a program intended to be highly parallelized are the scaling properties. We can now check these for the two sections of the code, the generation of configurations and the gradient flow. We will distinguish the analysis in the two usual quantities used in High Performance Computing: strong and weak scaling. \\
\subsubsection{Strong Scaling}
Strong scaling is the performance of a program as the total problem size is kept fixed and the number of parallel units, the number of processors in this case, increases. The parallel efficiency is computed as:
\beq
    \eta_s = \frac{t_0}{t_NN}\cdot 100\% 
    \label{eq:eta_strong}
\eeq 
where $t_0$ is the execution time for one unit of computation, $t_N$ the execution time for the case of $N$ units computing the same system size. In the case for perfect parallelization the efficiency is $100\%$ as an increase of some factor in the number of processors would decrease by the same factor the execution time. In real cases this is seldomly true, the execution time is in fact usually larger than expected. \\
The test was set up in this way:
\begin{itemize}
    \item Gauge Field Generation: one lattice of size $24^3\times 48$ has been randomly generated and $10^4$ MC cycles were performed. The system was split in sub-lattices of decreasing size, in order to increase the number of processors.
    \item Gradient Flow: ten lattices of size $24^3\times 48$ have been flowed for a total of $1000$ integration steps each. Again, the sub-lattice size was decreased and the number of processor increased accordingly. 
\end{itemize}

\fig[0.7]{implementation/StrongScaling.pdf}{Strong scaling of the two parts of the program. The efficiency, \cref{eq:eta_strong}, is normalized to the first point plotted, that is 64 processors.}{fig:strong_scaling}

Figure \ref{fig:strong_scaling} shows that the performance scales well with the number of processors, but the efficiency is dropping for large processor numbers. This is expected and the reason is that the surface to volume ratio of the sub-lattice decreases when more processors are added. This makes the fraction of operations that require inter-processor communication become comparable to the ``inner" points calculations. It is to be noted that smaller number of processors could not be used, as the processors in that case would have been all on the same node (the cluster used for testing has 28 cores per node), so any timing would have excluded the inter-node communication time, which is significantly higher than the inter-node one.\\
Overall the gradient flow program scales better than the ensemble generation one. This is most likely explained by the fact that the communication overhead introduced by \texttt{MPI} is larger in the case of the generation program (which shares one link variable at a time) than in the gradient flow code (which uses lattice shifts and shares one whole shared edge between processors at once, see \cref{sec:shift} for details).

\subsubsection{Weak Scaling}
By weak scaling of a parallelized problem it is intended the measure of the performance as the local workload is kept fixed and the total volume of the problem is increased by adding processors. The efficiency is computed as:
\beq
\eta_W = \frac{t_0}{t_N}\cdot 100\% 
\label{eq:eta_weak}
\eeq 
where $t_0$ is the execution time of the smallest case and $t_N$ the one of a system $N$ times larger computed on $N$ times the parallel units. The test setup was:
\begin{itemize}
    \item Gauge Field Generation: the sub-lattice has been fixed to $4^4$, and volumes from $16^3\times32$, a total of $32$ processes, to $32^3\times64$, $512$ processors, have been considered. Intermediate volumes are built by multiplying one spatial dimension at a time by a factor of 2. For each volume $10000$ MC cycles have been computed.
    \item Gradient Flow: the gauge fields generated in the previous point have been flowed for a total of $1000$ integration steps by the same number of processors that generated them, so the sub-lattice size was set again to be $4^4$.
\end{itemize}

\fig[0.7]{implementation/WeakScaling.pdf}{Weak scaling of the two parts of the program. The efficiency is the ratio of \cref{eq:eta_weak} with respect to the first data point. }{fig:weak_scaling}

The results in \cref{fig:weak_scaling} show that both the programs are perfectly scalable. The communication overhead, introduced by the need of sharing link variables across the edges, is almost constant with respect to the number of processors. We should note that already the first point plotted contains a large amount of communications between processors. What the plot is truly suggesting is that the addition of other processors keeps the overall overhead constant. This implies that the parallelization procedure has been implemented correctly, though it could still be potentially improved by further optimizations.

These results for strong and weak scaling are very comforting, as they show that the hardest part of the development process, the parallelization of the code, has been well designed and implemented.


\section{Main Runs and Timing}
To generate all the data that was needed for our analysis we had to perform eight main calculations. First we generated the four ensembles of table \cref{runs:ensembles}, by running the gauge field generator code. Afterwards the numerical integration of the gradient flow equation has been performed on all the four sets of gauge field configurations.\\
All runs of the programs, were carried out on the High Performance Computing Center at Michigan State University (MSU), with the support of the Institute for Cyber-Enabled Research (iCER). Development was performed on local machines and on the small cluster SMAUG located at the Department of Physics of the University of Oslo (UiO) and some larger benchmarks were run on the Abel Computer Cluster also at UiO.\\
The ensembles on which the results in \cref{part:results} are based were generated using the parameters in  \cref{runs:mcparams}. A summary of the computational resources used for the generation of all four ensembles is presented in \cref{runs:times}. 

\begin{table}[!htb]
    \capt{Execution times for generating ensembles from \cref{runs:ensembles} with parameters found in \cref{runs:mcparams}. All the runs were performed on the iCER cluster at MSU. $N_{procs}$ is the number of processors used for each ensemble; the Wall Time is the ``wall clock'' time (the time spent in the parallel execution); Time per MC cycle is the time spend in each step of the Markov Chain; Time per Configuration is the time spent between saved configurations, to be used for observables, so it includes the adjustment made for the longer autocorrelation times of larger $\beta$ values.} 
    \begin{center}
    \begin{tabular}{ccccc}
        $\beta$ & $N_{procs}$ & Wall Time [h] & Time per MC Cycle [s] & Time per Configuration [s]\\\hline
        $6.00$ & $1024$ & $27$ & $0.16104$  & $96.6$\\
        $6.10$ & $512$ & $35$ & $0.42102$ & $252.6$\\
        $6.20$ & $1024$ & $53$ & $0.4780825$ & $382.4$ \\
        $6.45$ & $1024$ & $204$ & $1.836$ & $2937.6$
    \end{tabular}
    \label{runs:times} 
    \end{center}
\end{table}

It is clear from the table that the $\beta=6.45$ ensemble, out of which only 250 configurations were saved, had execution times larger by one order of magnitude compared to the second largest; this made increasing the ensemble size unfeasible with the time and resources for this work.\\
To conclude, in \cref{runs:times_flow} we present the summary of the computational resources used by the Gradient Flow program to integrate the flow equation on all the configurations of the ensembles.

\begin{table}[!htb]
    \capt{Execution times for the numerical integration of the flow equations on the ensembles from \cref{runs:flow} with parameters found in \cref{runs:flow}. All the runs were performed on the iCER cluster at MSU. $N_{procs}$ is the number of processors used for each ensemble; the Wall Time is the ``wall clock'' time (the time spent in the parallel execution); Time per Configuration is the time spent flowing one single gauge configuration.} 
    \begin{center}
    \begin{tabular}{cccc}
        $\beta$ & $N_{procs}$ & Wall Time [h] & Time per Configuration [s]\\\hline
        $6.00$ & $1024$ & $15$ & $109.3$\\
        $6.10$ & $512$ & $30$ & $218.6$\\
        $6.20$ & $1024$ & $33$ & $243.6$ \\
        $6.45$ & $1024$ & $96$ & $1326.9$
    \end{tabular}
    \label{runs:times_flow} 
    \end{center}
\end{table}