One of the major focuses of this work has been a completely new implementation of a program to generate and analyze $SU(3)$ gauge fields. As it is common practice in Lattice QCD numerical implementations, the program is separated in two parts that are computationally intensive and one that is easier in that sense:
\begin{itemize}
    \item \textit{generation of gauge fields}: in this case it is done through a simple Metropolis algorithm using the standard Wilson action;
    \item \textit{computation of observables}: this includes applying the gradient flow as well as computing the energy density and the topological charge at every flow-time;
    \item \textit{computation of derived observables}: mainly post analysis, error analysis and model fits to data. 
\end{itemize}
Here we will present the main features of the first two steps, which are the most interesting ones. The programming language of choice is \cpp because of its high efficiency, high abstraction capabilities (the code-base is highly object oriented) and for the easiness of the \mpi integration. The analysis of data has been performed using \texttt{python} and in particular relying heavily on its standard data science packages such as \texttt{numpy} and \texttt{pandas}.
 
\section{Generating Pure Gauge Fields}
The task of generating lattice field configurations is extremely demanding in terms of computation requirements. The case of QCD is much more demanding than that of a pure Yang-mills theory, but overall the latter calculation is still challenging. The main, and perhaps overwhelmingly simple, reason for this problem is the dimensionality. Dealing with a discretized space-time lattice, things tend to scale with powers of $4$, a trivial example is cutting in half the lattice spacing keeping a fixed total volume: this requires $2^4$ more points in the global lattice. \\
To get a better feeling of the algorithm we first have to look at what the basic object of the program is: the lattice. The number of double precision floating point numbers to be stored for a field configuration is given by
\begin{equation}
    \underbrace{N^3}_\text{spatial dimension} \times
    \underbrace{N_t}_\text{time dimension} \times
    \underbrace{4}_\text{links per site} \times 
    \underbrace{9}_\text{SU(3) matrix size} \times
    \underbrace{2}_\text{real and imaginary part}
\end{equation}
this implies that, for example, if we choose $N=48,~N_t=96$ the resulting configuration is $6115295232~bytes$ large, that is $6~GBytes$.  This limits greatly the possibility of simulating large systems \\
The basic element at each lattice site is a set of $4$ different $SU(3)$ matrices, one for each dimension. These links are to be intended as the integral of the gauge field from one site to the adjacent one along each dimension. A more formal description has been given in chapter 3.\\
\subsection{The Metropolis Algorithm}
The main algorithm that has been used to generate an ensemble of gauge field configurations is the Metropolis Algorithm. It is a widely popular Markov Chain Monte Carlo Method to generate a sequence of random samples from a probability distribution. In our case the probability distribution is the gluon Wilson Action and our random samples are the gauge configurations themselves. A simplified version of the algorithm is described below:
\begin{algorithm}
    \caption{Metropolis Algorithm}\label{metropolis:algo}
    \begin{algorithmic}[1]
    \State $\textit{configuration} \gets \text{initial configuration}$
    \For {$i < MonteCarloCycles$}
        \State $newConfiguration  \gets \text{random move} + configuration$  
        \State $\Delta S \gets \text{action} (newConfiguration) - \text{action} (configuration)$
        \If {$\Delta S > random(0,1)$} 
            \State $configuration \gets newConfiguration$
        \EndIf
    \EndFor
    \end{algorithmic}
\end{algorithm}

The actual implementation of this algorithm on a lattice is however not as trivial as it seems, because we are dealing with a lattice gauge configuration, so we need to define what is a random move and what is the action difference for our specific case.

One configuration is saved, in the form of a simple binary containing all the data of the lattice, as an intermediate result every $N_C$ Monte Carlo updates. We need this in order to apply the gradient flow afterwards to the configurations and compute the observables we want. The choice of $N_C$ turned out to be crucial for the autocorrelation of some observables, in particular the topological charge.

\subsection{Sampling the Configuration Space}
To use Metropolis' algorithm we need to define what a random move is. Using the Wilson Action \LINK, which is defined on plaquettes, an update on a single link variable can be seen as a small unitary transformation. 
In order to generate such transformation use three random $SU(2)$ matrices "close to unity". By this expression we mean that the real diagonal components are the dominant terms of the matrix. \CIT 
\beq
    R_2 = \begin{pmatrix}
        r_{11} & r_{12} \\ r_{21} & r_{22} 
    \end{pmatrix}
    ~~~~~~~
    S_2 = \begin{pmatrix}
        s_{11} & s_{12} \\ s_{21} & s_{22} 
    \end{pmatrix}
    ~~~~~~~
    T_2 = \begin{pmatrix}
        t_{11} & t_{12} \\ t_{21} & t_{22} 
    \end{pmatrix}
\eeq
The elements of the matrix are chosen at random by choosing four random numbers $r_0$, $\vec{r}$ (a three component vector) between $(-\frac{1}{2}),\frac{1}{2})$. 
We then introduce a "spread parameter" $\epsilon$ that controls how much the off-diagonal terms will weight, so we scale our random variables by:
\beq
    x_0 = sign(r_0)\sqrt{1-\epsilon^2}~~~~~~\vec{x} = \epsilon \frac{\vec{r}}{|\vec{r}|}
\eeq
and we use these coefficients together with the generators of the $SU(2)$ group (the Pauli matrices) to build an element of the group:
\beq
U = x_0\mathds{1} + i\vec{x}\cdot\vec{\sigma} = \begin{pmatrix}
    u_{11} & u_{12} \\ u_{21} & u_{22} 
\end{pmatrix}
\eeq

We then embed these $SU(2)$ matrices in three $SU(3)$ matrices by mapping them as:
\beq
    R = \begin{pmatrix}
        r_{11} & r_{12} & 0\\ r_{21} & r_{22} & 0 \\ 0 & 0 & 1 
    \end{pmatrix}
    ~~~~~~~
    S = \begin{pmatrix}
        s_{11} & 0 & s_{12} \\ 0 & 1 & 0 \\ s_{21} & 0 & s_{22} 
    \end{pmatrix}
    ~~~~~~~
    T = \begin{pmatrix}
        1 & 0 & 0 \\ 0 & t_{11} & t_{12} \\ 0 & t_{21} & t_{22} 
    \end{pmatrix}
\eeq
These three matrices are clearly members of $SU(3)$ and so is their product $X = RST$. We thus have defined a recipe for numerically generating random group transformations, the key element for our algorithm.

An additional element that we need to define is the action difference $\Delta S$. On a single link $U_\mu(x)$ we apply a random transformation $X$ and get $U'_\mu(x) = XU_\mu(x)$. The total change in the action only depends on those plaquettes that contain the considered link variable. In four dimensions there are 12 such elements:
\beq
    \Delta S = S[U'_\mu(x)] - S[U_\mu(x)]  = -\frac{\beta}{N} \Re~ Tr [U'_\mu(x) - U_\mu(x)] A
\eeq
where $A$ is the sum of the "staples" of the link $U$. They are the constant three sides of the plaquettes that contain $U$:
\beq
    A = \sum_{\nu\neq\mu} \left[ U_{\nu}(x+\mu)U_{-\mu}(x+\mu+\nu)U_{-\nu}(x+\nu) +  U_{-\nu}(x+\mu)U_{-\mu}(x+\mu-\nu)U_{\nu}(x-\nu)  \right]
\eeq 
\NOTE{add simpler form}

\FIGURE{STAPLES AND LINKS}

\subsection{Updates Strategies}
There is now some arbitrariness in what is defined as an update. In this work we call an update the following procedure:
\begin{algorithm}
    \caption{Metropolis Update}\label{metropolis:update}
    \begin{algorithmic}[1]
    \For {$x,\mu$}
        \State $A \gets computeStaples(x,\mu)$     
        \For {$i < N_{H}$}
            \State $U_{new}(x,\mu)  \gets X\cdot U(x,\mu)$  
            \State $\Delta S \gets (U_{new}(x,\mu)  - U(x,\mu) )\cdot A$
            \If {$realTrace(\Delta S) > random(0,1)$} 
                \State $U(x,\mu)  \gets U_{new}(x,\mu)$
            \EndIf
        \EndFor
    \EndFor
\end{algorithmic}
\end{algorithm}

These updates are the ones we consider when we refer to Monte Carlo cycles, autocorrelation times and so on. Note that each update includes a loop over all links on the lattice and that every link is "hit" $N_H$ times before moving to the next one. This is done for computational efficiency, because computing the staples is the most expensive part of the algorithm, once $A$ is computed for a link, the result is used to attempt multiple updates on the link. It can also be shown that if $N_H$ is sufficiently large the algorithm becomes equivalent to the heatbath algorithm. \CIT

The order in which the links in the lattice are visited is also arbitrary. The simplest way, the ordered one, was adopted. This however might have impacted the autocorrelation of the system, not allowing significant modification to the system as an update depends on the neighbors. An alternative choice is a checkerboard pattern, which has potential benefits to the autocorrelation time of the observables as well as on the parallelization scheme.\NOTE{see discussion}. 

\subsection{Parallelization Scheme}
Given the size of the lattice (from $V \approx 10^5$ to $V\approx 10^7$, the total number of lattice sites), it is necessary to split the computation of the updates on more than one processors. The most direct way is to divide the lattice into sub-blocks and have each process handle its portion of the field alone. \FIGURE{blocks?} However, because of the dependence of the action difference of a link on its neighbors, when updating a link on the edge of of a sub-block information about another block is needed. Here is where the Message Passing Interface (MPI) comes in use. For this particular problem we decided to use a point to point communication scheme between the processors, the use of periodic boundary conditions allowed also for non-blocking communications to be used, in particular the collective geometry based scheme shown in \LINK has been implemented.
\FIGURE{sendrecv}
Note that the communication is relevant only for the computation of the staples, this is another argument in favor of performing multiple hits on a link at every update. It is clear to see that the algorithm can be affected by large communication overhead problems. If the sub-blocks are too small, then most of the time would be spent on sending and receiving links from the neighbors. This causes the execution time to depend not linearly on the number of processors, instead the relation flattens at some value given by the communication overhead.

\subsection{Summary of the Parameters}
In total the algorithm need four parameters as inputs. Of these, only one defines the physics of the system, the others have to be set and optimized in order to improve the acceptance ratio of the metropolis test and the autocorrelation of the observables computed on the generated configurations.

\begin{table}[!htb]
\begin{center}
\begin{tabular}{cl}
    Parameter & Description\\\hline
    $\beta$ & Coupling parameter of the Wilson Action\\
    $\epsilon$ & "Spread" of the random $SU(3)$ elements for the updates\\
    $N_C$ & Number of updates between one saved configuration (observable measurement)\\
    $N_H$ & Number of of "hits" per link at every update, with constant staple 
\end{tabular}
\label{MC:params}
\capt{Parameters for the Monte Carlo generation of Yang-Mills gauge field configurations via the Metropolis Algorithm}
\end{center}
\end{table}

\section{Wilson Flow of Gauge Configurations}
To study the flow-time dependence of the observables a numeric implementation of \LINK is needed. The main challenges here are to define the derivative of the action at a given lattice site and the numerical exponentiation of an $\mathfrak{su}(3)$ element, which returns an $SU(3)$ element.
\subsection{The Action Derivative}
Following the procedure in \CIT we define the derivative of the action at a lattice site $U_\mu(x)$ to be:
\beq
    \partial_\mu S[U_\mu(x)] = \frac{i}{2}\left(\Omega_\mu(x) - \frac{1}{3}\mathds{1}\Im~Tr[\Omega_\mu(x)]\right)
\eeq
with 
\beq    
    \Omega_\mu(x) = U_\mu(x) A_\mu(x) - A^\dagger_\mu(x) U^\dagger_\mu(x)
\eeq
where $A_\mu(x)$ are the staples of the link. We can note that the derivative is always traceless anti-hermitean matrix \NOTE{check...}, thus an element of $\mathfrak{su}(3)$ as expected.

\subsection{Exponential of a $\mathfrak{su}(3)$ Element}
Again following \CIT we provide a numerical recipe for taking the exponential function of a traceless anti-hermitean $3\times3$ matrix. The key idea is to use the Cayley-Hamilton theorem, 
\NOTE{da fare...}

\subsection{Parallelization Scheme}
Also this problem is very expensive in computational terms, so it is worth designing a parallelization scheme for it. The idea is still to split the lattice into sub-blocks and have each processor handle one of them.
By looking at the \LINK we see that all operations can be defined on a lattice-wise scale and that they all depend on one previous state of the field, not on an intermediate one as was the case for the generation of gauge fields. This allows us to define a "shift" operation, that effectively creates an additional lattice that is the translation of the original along an axis. The huge advantage is that in order to perform this operation only one communication instance is needed (following the previous scheme in \LINK), but the message is now a whole shared cube between to processors. \FIGURE{cubes being shared} \NOTE{ADD ALGO...} This has to be implemented for the action derivative lattice and the gauge field itself. The massive reduction in communication overhead improves the scaling of the algorithm greatly, making this part of the problem much more efficient.

\section{Structure and Tools}
The full code can be found on the web under the link \NOTE{github...}, where both the code for the generation and the flow of gauge fields is hosted. The technical documentation is found at \NOTE{documentation...}. As already mentioned the language of choice was \cpp, mainly because of the high-performance and abstraction level it  provides. An object-oriented structure has been used, as can be seen in \FIGURE{scheme class}. 
Notable tools that have been used that deserve a mention are \mpi, nlhoman json (used for easy input parameters handling via json files), cmake (for building the project) and valgrind for funcition and memory profiling.
